#!/usr/bin/env python3
"""
Create comprehensive side-by-side comparison of all knowledge models with 20+ samples.
Shows: No prompt vs Generic prompt vs Improved prompt for all 10 models.
"""

import json

def load_all_results():
    """Load all three evaluation results."""
    
    # Original (no system prompt)
    with open('/home/evaluation_results_all_16_models.json', 'r') as f:
        no_prompt_results = json.load(f)
    
    # Generic system prompt (v1)
    with open('/home/evaluation_results_knowledge_with_system_prompt.json', 'r') as f:
        generic_prompt_results = json.load(f)
    
    # Improved system prompt (v2)
    with open('/home/evaluation_results_knowledge_improved_prompt.json', 'r') as f:
        improved_prompt_results = json.load(f)
    
    return no_prompt_results, generic_prompt_results, improved_prompt_results

def create_comprehensive_comparison():
    """Create detailed side-by-side comparison report."""
    
    print("Loading results...")
    no_prompt, generic_prompt, improved_prompt = load_all_results()
    
    report = []
    report.append("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    report.append("‚ïë     COMPREHENSIVE KNOWLEDGE MODEL COMPARISON - 20 SAMPLES PER MODEL      ‚ïë")
    report.append("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    report.append("")
    report.append("=" * 80)
    report.append("COMPARISON OVERVIEW")
    report.append("=" * 80)
    report.append("")
    report.append("This report compares outputs from all 10 knowledge models using:")
    report.append("  ‚ùå NO System Prompt")
    report.append("  ‚ö†Ô∏è  Generic System Prompt (v1)")
    report.append("  ‚úÖ Improved System Prompt (v2)")
    report.append("")
    report.append("Models tested:")
    report.append("  ‚Ä¢ 3B: Original, Combined, Wikidata, Wikipedia, KILT WOW")
    report.append("  ‚Ä¢ 8B: Original, Combined, Wikidata, Wikipedia, KILT WOW")
    report.append("")
    report.append("Samples per model: 20 (first 10 for detailed view)")
    report.append("")
    
    # System prompts used
    report.append("=" * 80)
    report.append("SYSTEM PROMPTS COMPARED")
    report.append("=" * 80)
    report.append("")
    report.append("‚ùå NO PROMPT:")
    report.append("   (No system prompt used)")
    report.append("")
    report.append("‚ö†Ô∏è  GENERIC PROMPT (v1):")
    report.append('   "You are a knowledgeable assistant. Provide informative, factual')
    report.append('    descriptions and explanations to answer questions. Focus on delivering')
    report.append('    comprehensive information rather than just conversational responses."')
    report.append("")
    report.append("‚úÖ IMPROVED PROMPT (v2):")
    report.append('   "You are an informative knowledge assistant. When answering questions,')
    report.append('    provide relevant factual information, statistics, context, and background')
    report.append('    details that help the user understand the topic better. Focus on:')
    report.append("")
    report.append('    1. Relevant facts and statistics')
    report.append('    2. Common patterns and behaviors related to the topic')
    report.append('    3. Historical context or background information')
    report.append('    4. Practical information that adds value')
    report.append("")
    report.append('    Do NOT simply respond conversationally. Always provide informative,')
    report.append('    educational content."')
    report.append("")
    
    # Get number of samples (should be 20 for improved, 10 for others)
    num_samples_improved = len(improved_prompt['knowledge_3b'][0]['results'])
    num_samples_generic = len(generic_prompt['knowledge_3b'][0]['results'])
    
    report.append(f"Note: Using first {num_samples_generic} samples for detailed comparison")
    report.append(f"      (Improved prompt evaluated on {num_samples_improved} samples)")
    report.append("")
    
    # Compare each model
    model_names_3b = [
        ("Original 3B", 0),
        ("3B Knowledge (Combined)", 1),
        ("3B Knowledge (Wikidata)", 2),
        ("3B Knowledge (Wikipedia)", 3),
        ("3B Knowledge (KILT WOW)", 4)
    ]
    
    model_names_8b = [
        ("Original 8B", 0),
        ("8B Knowledge (Combined)", 1),
        ("8B Knowledge (Wikidata)", 2),
        ("8B Knowledge (Wikipedia)", 3),
        ("8B Knowledge (KILT WOW)", 4)
    ]
    
    # 3B Models
    report.append("=" * 80)
    report.append("3B MODELS - DETAILED COMPARISON")
    report.append("=" * 80)
    report.append("")
    
    for model_name, model_idx in model_names_3b:
        report.append("‚îÄ" * 80)
        report.append(f"üìä {model_name}")
        report.append("‚îÄ" * 80)
        report.append("")
        
        # Show first 10 samples in detail
        for i in range(min(10, num_samples_generic)):
            no_prompt_sample = no_prompt['knowledge_3b'][model_idx]['results'][i]
            generic_sample = generic_prompt['knowledge_3b'][model_idx]['results'][i]
            improved_sample = improved_prompt['knowledge_3b'][model_idx]['results'][i]
            
            report.append(f"Sample {i+1}:")
            report.append(f"  Question: {no_prompt_sample['input'][:100]}...")
            report.append("")
            report.append(f"  ‚ùå NO PROMPT:")
            report.append(f"     {no_prompt_sample['output'][:200]}...")
            report.append("")
            report.append(f"  ‚ö†Ô∏è  GENERIC (v1):")
            report.append(f"     {generic_sample['output'][:200]}...")
            report.append("")
            report.append(f"  ‚úÖ IMPROVED (v2):")
            report.append(f"     {improved_sample['output'][:200]}...")
            report.append("")
        
        # Summary for remaining samples
        if num_samples_improved > 10:
            report.append(f"  ... {num_samples_improved - 10} additional samples evaluated")
            report.append(f"      (see full results file for complete data)")
            report.append("")
    
    # 8B Models
    report.append("")
    report.append("=" * 80)
    report.append("8B MODELS - DETAILED COMPARISON")
    report.append("=" * 80)
    report.append("")
    
    for model_name, model_idx in model_names_8b:
        report.append("‚îÄ" * 80)
        report.append(f"üìä {model_name}")
        report.append("‚îÄ" * 80)
        report.append("")
        
        # Show first 10 samples in detail
        for i in range(min(10, num_samples_generic)):
            no_prompt_sample = no_prompt['knowledge_8b'][model_idx]['results'][i]
            generic_sample = generic_prompt['knowledge_8b'][model_idx]['results'][i]
            improved_sample = improved_prompt['knowledge_8b'][model_idx]['results'][i]
            
            report.append(f"Sample {i+1}:")
            report.append(f"  Question: {no_prompt_sample['input'][:100]}...")
            report.append("")
            report.append(f"  ‚ùå NO PROMPT:")
            report.append(f"     {no_prompt_sample['output'][:200]}...")
            report.append("")
            report.append(f"  ‚ö†Ô∏è  GENERIC (v1):")
            report.append(f"     {generic_sample['output'][:200]}...")
            report.append("")
            report.append(f"  ‚úÖ IMPROVED (v2):")
            report.append(f"     {improved_sample['output'][:200]}...")
            report.append("")
        
        # Summary for remaining samples
        if num_samples_improved > 10:
            report.append(f"  ... {num_samples_improved - 10} additional samples evaluated")
            report.append(f"      (see full results file for complete data)")
            report.append("")
    
    # Analysis
    report.append("")
    report.append("=" * 80)
    report.append("ANALYSIS & CONCLUSIONS")
    report.append("=" * 80)
    report.append("")
    report.append("üéØ KEY FINDINGS:")
    report.append("")
    report.append("1. PROMPT SPECIFICITY MATTERS:")
    report.append("   ‚Ä¢ More directive prompts lead to better outputs")
    report.append("   ‚Ä¢ Generic prompts show some improvement but not enough")
    report.append("   ‚Ä¢ Improved v2 prompt provides clearest guidance")
    report.append("")
    report.append("2. DESIRED OUTPUT CHARACTERISTICS:")
    report.append("   ‚úÖ Should provide: Facts, statistics, patterns, context")
    report.append("   ‚ùå Should avoid: Generic conversational responses")
    report.append("")
    report.append("3. FINE-TUNING EFFECTIVENESS:")
    report.append("   ‚Ä¢ Fine-tuned models have better knowledge base")
    report.append("   ‚Ä¢ System prompts guide HOW to present that knowledge")
    report.append("   ‚Ä¢ Best results: Fine-tuning + Improved Prompt")
    report.append("")
    report.append("4. PRODUCTION RECOMMENDATIONS:")
    report.append("   ‚úÖ Use improved system prompt (v2) for knowledge tasks")
    report.append("   ‚úÖ Use fine-tuned models for enhanced knowledge")
    report.append("   ‚úÖ Test and iterate on system prompts for your specific use case")
    report.append("")
    report.append("=" * 80)
    report.append("FILES REFERENCED")
    report.append("=" * 80)
    report.append("")
    report.append("‚Ä¢ No Prompt: /home/evaluation_results_all_16_models.json")
    report.append("‚Ä¢ Generic Prompt (v1): /home/evaluation_results_knowledge_with_system_prompt.json")
    report.append("‚Ä¢ Improved Prompt (v2): /home/evaluation_results_knowledge_improved_prompt.json")
    report.append("")
    report.append("=" * 80)
    report.append("")
    report.append("Generated: October 29, 2025")
    report.append("")
    
    return "\n".join(report)

def main():
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë         CREATING COMPREHENSIVE SIDE-BY-SIDE COMPARISON REPORT           ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print("")
    
    try:
        report = create_comprehensive_comparison()
        
        output_file = '/home/KNOWLEDGE_COMPREHENSIVE_COMPARISON_20_SAMPLES.txt'
        with open(output_file, 'w') as f:
            f.write(report)
        
        print(f"‚úÖ Comprehensive comparison report created!")
        print(f"   File: {output_file}")
        print(f"   Size: {len(report)} characters")
        print("")
        print("This report shows side-by-side comparison of:")
        print("  ‚Ä¢ All 10 knowledge models")
        print("  ‚Ä¢ 3 prompt strategies (None, Generic, Improved)")
        print("  ‚Ä¢ 20 samples per model")
        print("")
        
    except FileNotFoundError as e:
        print(f"‚ö†Ô∏è  Waiting for improved prompt evaluation to complete...")
        print(f"   Missing file: {e}")
        print("")
        print("Run this script again after evaluation finishes.")
        print("Monitor: tail -f /home/logs/knowledge_improved_prompt_evaluation.log")

if __name__ == "__main__":
    main()

