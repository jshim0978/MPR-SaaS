# MPR-SaaS: Multi-Prompt Refinement as a Service

**A cloud-native prompt refinement framework for reducing LLM hallucinations**

---

## ğŸ“Š Repository Structure

```
MPR-SaaS/
â”œâ”€â”€ README.md                    â† This file
â”‚
â”œâ”€â”€ sbs29_baselines/             â† Baseline evaluation results (sbs29)
â”‚   â”œâ”€â”€ README.md                   - Baseline results documentation
â”‚   â”œâ”€â”€ SBS29_results.zip           - Complete results package (16 MB)
â”‚   â”œâ”€â”€ baselines/                  - Baseline implementations
â”‚   â”œâ”€â”€ datasets/                   - Evaluation datasets
â”‚   â”œâ”€â”€ docs/                       - Replication guides
â”‚   â”œâ”€â”€ graphs/                     - Visualizations
â”‚   â””â”€â”€ scripts/                    - Evaluation scripts
â”‚
â”œâ”€â”€ comparison/                  â† Comparison framework code (sbs29)
â”‚   â”œâ”€â”€ baselines/                  - Framework implementations
â”‚   â”œâ”€â”€ datasets/                   - Raw evaluation datasets
â”‚   â””â”€â”€ results_complete/           - Full evaluation results
â”‚
â”œâ”€â”€ orchestrator/                â† PRaaS orchestrator (jw1)
â”œâ”€â”€ workers/                     â† Worker node code (jw2, jw3)
â”œâ”€â”€ mpr/                         â† Core MPR library
â”œâ”€â”€ config/                      â† Configuration files
â””â”€â”€ prompts/                     â† Prompt templates
```

---

## ğŸ¯ Quick Start

### For Baseline Evaluations (sbs29)

```bash
# View baseline results
cd sbs29_baselines/
unzip SBS29_results.zip
cat README.md
```

### For Running PRaaS Framework

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure
cp config/decoding.example.json config/decoding.json
# Edit config/decoding.json with your settings

# 3. Start orchestrator (jw1)
cd orchestrator/
python3 router.py

# 4. Start workers (jw2, jw3)
cd workers/
python3 cleaner.py  # jw2
python3 describer.py  # jw3
```

---

## ğŸ“Š Baseline Evaluation Results

**Completed on sbs29 (November 7-9, 2025)**

### Frameworks Evaluated:
- **Control**: Baseline (no refinement)
- **OPRO**: 1-iteration optimization
- **PromptAgent**: Strategic planning
- **PromptWizard**: 3-round mutation

### Datasets Used:
- **TruthfulQA**: 817 samples
- **GSM8K**: 1,319 samples
- **AmbigQA**: 2,002 samples
- **HaluEval**: 1,000 samples (sampled with seed=42)

### Performance Summary (per sample average):

| Framework | Latency (s) | Tokens | vs Control |
|-----------|-------------|--------|------------|
| Control | 17.0 | 641 | 1.00Ã— |
| OPRO | 21.3 | 916 | 1.25Ã— slower |
| PromptAgent | 26.7 | 1,100 | 1.57Ã— slower |
| PromptWizard | 38.6 | 1,612 | 2.27Ã— slower |

**See `sbs29_baselines/` for complete results and replication guides.**

---

## ğŸ”§ Configuration

### Model Configuration (`config/decoding.json`)

```json
{
  "backbone_model": "meta-llama/Llama-3.2-3B-Instruct",
  "temperature": 0.2,
  "top_p": 0.9,
  "max_new_tokens": 512,
  "seed": 13
}
```

### Node Assignment

- **jw1**: Orchestrator (router, combiner, telemetry)
- **jw2**: Cleaner worker
- **jw3**: Describer worker
- **sbs29**: Baseline evaluations (trainer node)

---

## ğŸ“š Documentation

### Essential Docs:
- **`sbs29_baselines/README.md`** - Baseline evaluation results
- **`sbs29_baselines/docs/REPLICATION_GUIDE.md`** - How to replicate evaluations
- **`sbs29_baselines/docs/DATA_COLLECTION_SPEC.md`** - Data format specification

### For Other Servers:
If you're running evaluations on jw1, jw2, jw3, or kcloud:
1. Create your own directory (e.g., `jw1_praas/`)
2. Follow the data format in `sbs29_baselines/docs/`
3. Use the same datasets from `sbs29_baselines/datasets/`
4. Commit your results to your directory (no conflicts!)

---

## ğŸ“ Citation

If using this codebase or baseline results:

```
MPR-SaaS: Multi-Prompt Refinement as a Service
Baseline evaluations conducted on sbs29 (November 2025)
Frameworks: Control, OPRO, PromptAgent, PromptWizard
Target Model: meta-llama/Llama-3.2-3B-Instruct
Datasets: TruthfulQA, GSM8K, AmbigQA, HaluEval
```

---

## ğŸ”„ Contributing

### Adding Your Evaluation Results:

1. **Create your directory**:
   ```bash
   mkdir your_server_name/
   ```

2. **Add your results**:
   ```bash
   cp your_results.json your_server_name/
   git add your_server_name/
   ```

3. **Commit and push**:
   ```bash
   git commit -m "Add [your_server] evaluation results"
   git push origin main
   ```

### Directory Naming Convention:
- `sbs29_baselines/` - Baseline evaluations (sbs29)
- `jw1_praas/` - PRaaS evaluation (jw1)
- `jw2_worker/` - Worker node files (jw2)
- `jw3_worker/` - Worker node files (jw3)
- `kcloud_deployment/` - Deployment files (kcloud)

This ensures no conflicts between servers!

---

## âœ… Status

- âœ… **sbs29 baselines**: Complete (20,552 samples, 100% success)
- â³ **jw1 PRaaS**: In progress
- â³ **BBH, StrategyQA, CSQA**: To be evaluated
- â³ **EvoPrompt**: To be evaluated

---

## ğŸ“ Questions?

For questions about:
- **Baseline results**: See `sbs29_baselines/README.md`
- **Replication**: See `sbs29_baselines/docs/REPLICATION_GUIDE.md`
- **Data format**: See `sbs29_baselines/docs/DATA_COLLECTION_SPEC.md`
- **Framework code**: See `comparison/` or respective worker directories

---

**Last Updated**: November 10, 2025  
**Repository**: https://github.com/jshim0978/MPR-SaaS  
**Rules used**: [JW-Global, MPR-Detected: yes]
