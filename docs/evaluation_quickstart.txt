╔═══════════════════════════════════════════════════════════════════════════╗
║                   🚀 QUICK START - MODEL EVALUATION                        ║
╚═══════════════════════════════════════════════════════════════════════════╝

█ START EVALUATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    /home/run_model_evaluation.sh

Time: ~30-45 minutes
Samples: 10 per task (Grammar, Paraphrase, Knowledge)
Models: 6 fine-tuned models compared with originals

█ VIEW RESULTS AGAIN
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    python3 /home/compare_evaluation_results.py

Or read the saved report:

    cat /home/evaluation_comparison_report.txt

█ OUTPUT FILES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

JSON (machine-readable):
  • /home/evaluation_results_grammar.json
  • /home/evaluation_results_paraphrase.json
  • /home/evaluation_results_knowledge.json

Text (human-readable):
  • /home/evaluation_comparison_report.txt

█ EXPAND TO 100 SAMPLES
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Edit /home/evaluate_all_models.py:

Line 71, 126, 181 - Change from:
    test_subset = test_samples[:10]
To:
    test_subset = test_samples

Then re-run:
    /home/run_model_evaluation.sh

Time: ~3-4 hours

█ MODELS BEING TESTED
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Grammar (JFLEG):
  ✓ Llama 3.2 3B - Original vs Fine-tuned
  ✓ Llama 3.1 8B - Original vs Fine-tuned

Paraphrase (PAWS):
  ✓ Llama 3.2 3B - Original vs Fine-tuned
  ✓ Llama 3.1 8B - Original vs Fine-tuned

Knowledge (Wikipedia):
  ✓ Llama 3.2 3B - Original vs Fine-tuned
  ✓ Llama 3.1 8B - Original vs Fine-tuned

█ DOCUMENTATION
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Full guide:
    cat /home/MODEL_EVALUATION_GUIDE.md

═══════════════════════════════════════════════════════════════════════════

