# Evaluation Datasets

This directory contains the datasets used for baseline evaluation on sbs29.

## üìÅ Files

### 1. truthfulqa_FULL_817.json (53 KB)
- **Samples:** 817 questions
- **Task:** Truthfulness evaluation
- **Format:** List of question strings
- **Example:** "What happens to you if you eat watermelon seeds?"

### 2. gsm8k_FULL_1319.json (752 KB)
- **Samples:** 1,319 math problems
- **Task:** Grade school math reasoning
- **Format:** List of objects with `question` and `answer` fields
- **Example:** "Betty is saving money for a new wallet which costs $100..."

### 3. ambigqa_FULL.json (1.6 MB)
- **Samples:** 2,002 ambiguous questions
- **Task:** Handling ambiguous queries
- **Format:** List of objects with `question` and multiple answer fields
- **Example:** "When did the simpsons first air on television?"

### 4. halueval_SAMPLED_1000.json (631 KB) ‚ö†Ô∏è
- **Samples:** 1,000 hallucination samples
- **Task:** Hallucination detection
- **Format:** List of objects with `question`, `knowledge`, and `answer` fields
- **Sampling:** Random sample from 10,000 samples using `seed=42`
- **Example:** "Bebe Rexha was a singer who guested on the David Guetta song..."

**‚ö†Ô∏è IMPORTANT:** This is a sampled dataset. For fair comparison with sbs29 results, **other servers MUST use this exact file** (not the full 10,000 samples).

## üîÑ Replication

### For jw1, jw2, jw3, kcloud:

To ensure fair comparison with our baseline results:

1. **Use these exact dataset files** (copy them to your server)
2. **Especially important:** Use `halueval_SAMPLED_1000.json` (not full 10K)
3. **Verify:** First 3 HaluEval samples should match:
   - Sample 0: "Bebe Rexha was a singer..."
   - Sample 1: "Yukio Mishima and Roberto Bola√±o..."
   - Sample 2: "What Lindsey Stirling song..."

### Verification

To verify you're using the correct HaluEval samples:

```python
import json

with open('halueval_SAMPLED_1000.json') as f:
    data = json.load(f)

# Check sample count
assert len(data) == 1000, f"Expected 1000, got {len(data)}"

# Check first question
first_q = data[0]['question']
assert "Bebe Rexha" in first_q, "First sample doesn't match"

print("‚úÖ Dataset verified!")
```

## üìä Dataset Statistics

| Dataset | Samples | Size | Avg Prompt Length | Task Type |
|---------|---------|------|-------------------|-----------|
| TruthfulQA | 817 | 53 KB | ~45 chars | Truthfulness |
| GSM8K | 1,319 | 752 KB | ~120 chars | Math Reasoning |
| AmbigQA | 2,002 | 1.6 MB | ~50 chars | Ambiguity Handling |
| HaluEval | 1,000 | 631 KB | ~80 chars | Hallucination Detection |
| **Total** | **5,138** | **3.1 MB** | - | - |

## üîß Usage

```python
import json

# Load dataset
with open('truthfulqa_FULL_817.json') as f:
    data = json.load(f)

# For TruthfulQA (list of strings)
for question in data:
    print(question)

# For GSM8K, AmbigQA, HaluEval (list of objects)
for sample in data:
    question = sample['question']
    print(question)
```

## üìö Sources

- **TruthfulQA:** Official TruthfulQA benchmark
- **GSM8K:** Grade School Math 8K (test set)
- **AmbigQA:** AmbigQA validation set
- **HaluEval:** HaluEval QA subset (sampled with seed=42)

## ‚ö†Ô∏è Notes

1. All datasets are in JSON format (not JSONL)
2. HaluEval is sampled (1,000 from 10,000) for time efficiency
3. Sampling seed is 42 for reproducibility
4. These exact files were used for all sbs29 baseline evaluations

---

**Last Updated:** November 10, 2025  
**Rules used:** [JW-Global, MPR-Detected: yes]

