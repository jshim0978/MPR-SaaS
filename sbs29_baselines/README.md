# SBS29 Baseline Evaluations

**Trainer Node:** sbs29  
**Evaluation Period:** November 7-9, 2025  
**Status:** âœ… Complete (20,552 samples, 100% success rate)

## ğŸ“Š Overview

This directory contains the complete baseline evaluation results for our EACL manuscript. We evaluated 4 prompt optimization frameworks (Control, OPRO, PromptAgent, PromptWizard) across 4 datasets (TruthfulQA, GSM8K, AmbigQA, HaluEval) using Llama-3.2-3B-Instruct as the target model.

## ğŸ“ Directory Structure

```
sbs29_baselines/
â”œâ”€â”€ README.md                    â† This file
â”œâ”€â”€ EVALUATION_RESULTS.md        â† Detailed results summary
â”œâ”€â”€ SBS29_results.zip            â† Complete results package (16 MB)
â”‚
â”œâ”€â”€ baselines/                   â† Baseline implementations
â”‚   â”œâ”€â”€ opro_baseline.py         â† OPRO (Optimization by PROmpting)
â”‚   â”œâ”€â”€ promptagent_baseline.py  â† PromptAgent (Strategic planning)
â”‚   â””â”€â”€ promptwizard_baseline.py â† PromptWizard (Mutation-based)
â”‚
â”œâ”€â”€ datasets/                    â† Evaluation datasets
â”‚   â”œâ”€â”€ README.md                â† Dataset documentation
â”‚   â”œâ”€â”€ truthfulqa_FULL_817.json â† 817 questions
â”‚   â”œâ”€â”€ gsm8k_FULL_1319.json     â† 1,319 math problems
â”‚   â”œâ”€â”€ ambigqa_FULL.json        â† 2,002 ambiguous questions
â”‚   â””â”€â”€ halueval_SAMPLED_1000.json â† 1,000 hallucination samples (seed=42)
â”‚
â”œâ”€â”€ docs/                        â† Documentation
â”‚   â”œâ”€â”€ REPLICATION_GUIDE.md     â† How to replicate our evaluation
â”‚   â”œâ”€â”€ DATA_COLLECTION_SPEC.md  â† Data format specification
â”‚   â””â”€â”€ QUICK_REFERENCE.txt      â† One-page quick reference
â”‚
â”œâ”€â”€ graphs/                      â† Visualizations (300 DPI PNG)
â”‚   â”œâ”€â”€ latency_comparison.png
â”‚   â”œâ”€â”€ token_usage_comparison.png
â”‚   â”œâ”€â”€ latency_breakdown.png
â”‚   â”œâ”€â”€ speedup_comparison.png
â”‚   â””â”€â”€ token_efficiency.png
â”‚
â””â”€â”€ scripts/                     â† Evaluation scripts
    â”œâ”€â”€ run_full_evaluation.py   â† Main evaluation runner
    â”œâ”€â”€ generate_graphs.py       â† Graph generation
    â””â”€â”€ aggregate_results.py     â† Results aggregation
```

## ğŸ¯ Quick Start

### 1. View Results

Extract and explore the complete results:

```bash
cd sbs29_baselines
unzip SBS29_results.zip
cat EVALUATION_RESULTS.md
```

### 2. Replicate Evaluation

See `docs/REPLICATION_GUIDE.md` for detailed instructions on replicating our evaluation on other servers (jw1, jw2, jw3, kcloud).

### 3. Generate Graphs

```bash
cd scripts
python3 generate_graphs.py
```

## ğŸ“Š Results Summary

### Frameworks Evaluated:
- **Control**: Baseline (no refinement)
- **OPRO**: 1-iteration optimization
- **PromptAgent**: Strategic planning
- **PromptWizard**: 3-round mutation

### Datasets Used:
- **TruthfulQA**: 817 samples
- **GSM8K**: 1,319 samples
- **AmbigQA**: 2,002 samples
- **HaluEval**: 1,000 samples (sampled with seed=42)

### Performance (average per sample):
| Framework | Latency (s) | Tokens | Slowdown | Token Overhead |
|-----------|-------------|--------|----------|----------------|
| Control | 17.0 | 641 | 1.00Ã— | 1.00Ã— |
| OPRO | 21.3 | 916 | 1.25Ã— | 1.43Ã— |
| PromptAgent | 26.7 | 1,100 | 1.57Ã— | 1.72Ã— |
| PromptWizard | 38.6 | 1,612 | 2.27Ã— | 2.51Ã— |

## ğŸ”§ Configuration

**Target Model:** `meta-llama/Llama-3.2-3B-Instruct`  
**Decoding:**
- Temperature: 0.2
- Top-p: 0.9
- Max tokens: 512
- Seed: 13

## ğŸ“¦ Complete Results Package

The `SBS29_results.zip` file (16 MB) contains:

- **10 detailed examples** with complete prompts and outputs
- **71 MB of raw JSON results** (20,552 samples)
- **5 publication-ready graphs** (300 DPI PNG)
- **Complete documentation** for replication
- **All datasets** used in evaluation

See the `README_FOR_ZIP.txt` inside the zip file for details.

## ğŸ”„ For Other Servers

If you're running PRaaS evaluation on jw1, jw2, jw3, or kcloud:

1. **Use the same datasets** (especially `halueval_SAMPLED_1000.json`)
2. **Match the LLM config** (temperature, seed, etc.)
3. **Follow the data format** in `docs/DATA_COLLECTION_SPEC.md`
4. **Save results** in your own server directory (e.g., `/home/jw1_praas/`)

This ensures fair comparison and no git conflicts!

## ğŸ“š Documentation

- **EVALUATION_RESULTS.md**: Detailed results summary with examples
- **docs/REPLICATION_GUIDE.md**: Step-by-step replication guide
- **docs/DATA_COLLECTION_SPEC.md**: JSON schema and data format
- **docs/QUICK_REFERENCE.txt**: One-page quick reference

## ğŸ“ Citation

If using these results in your manuscript:

```
Baseline evaluations conducted on sbs29 trainer node
November 7-9, 2025
Frameworks: Control, OPRO, PromptAgent, PromptWizard
Target Model: meta-llama/Llama-3.2-3B-Instruct
Datasets: TruthfulQA (817), GSM8K (1,319), AmbigQA (2,002), HaluEval (1,000)
```

## âœ… Verification

Before using these results:
- âœ… All 16 runs completed successfully
- âœ… 20,552 samples with complete data
- âœ… 100% success rate
- âœ… All metrics captured (latency, tokens, outputs)
- âœ… HaluEval sampled with seed=42 for reproducibility

## ğŸ“ Questions?

See the documentation in `docs/` or the complete package in `SBS29_results.zip`.

---

**Generated:** November 10, 2025  
**Rules used:** [JW-Global, MPR-Detected: yes]

